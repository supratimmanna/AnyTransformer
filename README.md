# AnyTransformer
It'll contain the smallest possible block of a transformer architecture so that we can build any transformer type model using those blocks.
--
Attention: MultiHead Self-Attention, MultiHead Group Query Attention
//
LoRA layer for PEFT: LoRA and DoRa

